{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Keyword extractor and summarizer</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ressources \n",
    "Unicode : https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n",
    "Rake : https://github.com/vgrabovets/multi_rake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword extractor perso = every word except stop_words -> lemmatizer to apply to keep only the root\n",
    "Keyword extractor summa = words more pertinent -> warning with summary test (no Japon)...\n",
    "Keyword extractor Rake = words more pertinent -> same as summa, no Japon..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords= lower, strip, token, summa-keywords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('french')\n",
    "new_stop_words = ['plus','quels','quel','quelle','quelles','tout','toutes']\n",
    "for word in new_stop_words:\n",
    "    stop_words.append(word)\n",
    "def keywordizer(path,encoding):\n",
    "    import re\n",
    "    text = open(path,'r',encoding=encoding)\n",
    "    text = text.read()\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub('(@\\S+)|\\s\\s+|[^\\w\\s]|\\d+',' ',text) #\\d+ = suppr numbers but what about dates, WW2 ?\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = [word for word in text if len(word)>1]\n",
    "    import numpy as np\n",
    "    text = np.unique(text)\n",
    "    return print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "au, aux, avec, ce, ces, dans, de, des, du, elle, en, et, eux, il, ils, je, la, le, les, leur, lui, ma, mais, me, même, mes, moi, mon, ne, nos, notre, nous, on, ou, par, pas, pour, qu, que, qui, sa, se, ses, son, sur, ta, te, tes, toi, ton, tu, un, une, vos, votre, vous, c, d, j, l, à, m, n, s, t, y, été, étée, étées, étés, étant, étante, étants, étantes, suis, es, est, sommes, êtes, sont, serai, seras, sera, serons, serez, seront, serais, serait, serions, seriez, seraient, étais, était, étions, étiez, étaient, fus, fut, fûmes, fûtes, furent, sois, soit, soyons, soyez, soient, fusse, fusses, fût, fussions, fussiez, fussent, ayant, ayante, ayantes, ayants, eu, eue, eues, eus, ai, as, avons, avez, ont, aurai, auras, aura, aurons, aurez, auront, aurais, aurait, aurions, auriez, auraient, avais, avait, avions, aviez, avaient, eut, eûmes, eûtes, eurent, aie, aies, ait, ayons, ayez, aient, eusse, eusses, eût, eussions, eussiez, eussent, plus, quels, quel, quelle, quelles, tout, toutes\n"
     ]
    }
   ],
   "source": [
    "print(', '.join([str(elem) for elem in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anecdotes' 'bento' 'cartes' 'comment' 'compose' 'coup' 'coutumes'\n",
      " 'cuisine' 'découverte' 'emballer' 'entièrement' 'façonner' 'fréquents'\n",
      " 'garnir' 'gestes' 'histoires' 'illustré' 'ingrédients' 'invitation'\n",
      " 'izakaya' 'japonais' 'japonaise' 'japonaises' 'livre' 'miso' 'plats'\n",
      " 'pose' 'questions' 'recette' 'recettes' 'repas' 'réponses' 'saveurs'\n",
      " 'savoir' 'soupe' 'sushi' 'techniques' 'traditionnelle' 'ustensiles' 'œil']\n"
     ]
    }
   ],
   "source": [
    "keywordizer('resume_cuisine-jap.txt','utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comment',\n",
       " 'toutes',\n",
       " 'recettes',\n",
       " 'recette',\n",
       " 'japonaise',\n",
       " 'japonaises',\n",
       " 'entièrement',\n",
       " 'illustré',\n",
       " 'tout',\n",
       " 'questions',\n",
       " 'ustensiles',\n",
       " 'ingrédients']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('resume_cuisine-jap.txt','r',encoding='utf-8')\n",
    "text = text.read()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('french')\n",
    "text = text.split()\n",
    "text = [word for word in text if word not in stop_words]\n",
    "text = ' '.join(text)\n",
    "from summa import keywords\n",
    "keywords = keywords.keywords(text)\n",
    "keywords.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('recettes', 1.0), ('anecdotes', 1.0), ('histoires', 1.0), ('cartes', 1.0), ('gestes', 1.0), ('techniques', 1.0), ('ustensiles', 1.0), ('ingrédients', 1.0), ('plats', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "from multi_rake import Rake\n",
    "text = open('resume_cuisine-jap.txt','r',encoding='utf-8')\n",
    "text = text.read()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('french')\n",
    "text = text.split()\n",
    "text = [word for word in text if word not in stop_words]\n",
    "text = ' '.join(text)\n",
    "r = Rake(max_words=1)\n",
    "keywords = r.apply(text)\n",
    "print(keywords[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "def summarizer(path,encoding):\n",
    "\tdocx = open(path,'r',encoding=encoding)\n",
    "\tdocx = docx.read()\n",
    "\tparser = PlaintextParser.from_string(docx,Tokenizer(\"french\"))\n",
    "\tlex_summarizer = LexRankSummarizer()\n",
    "\tsummary = lex_summarizer(parser.document,3)\n",
    "\tsummary_list = [str(sentence) for sentence in summary]\n",
    "\tresult = ' '.join(summary_list)\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffUn livre entièrement illustré pour tout savoir sur la cuisine japonaise ! Des recettes, des anecdotes, des histoires, des cartes, des gestes, des techniques, des ustensiles, des ingrédients… Une invitation à la découverte des coutumes et des saveurs japonaises !Comment façonner les sushi ? Quelle est la recette traditionnelle de la soupe miso ?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer('resume_cuisine-jap.txt','utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Pkgs\n",
    "import spacy \n",
    "import re\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "# Pkgs for Normalizing Text\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "# Import Heapq for Finding the Top N Sentences\n",
    "from heapq import nlargest\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('french')\n",
    "\n",
    "def text_summarizer(path,encoding):\n",
    "    raw_docx = open(path,'r',encoding=encoding)\n",
    "    raw_docx = raw_docx.read().strip()\n",
    "    import re\n",
    "    raw_docx = re.sub('\\n+',' ',raw_docx)#delete line break\n",
    "    docx = nlp(raw_docx)\n",
    "    # Build Word Frequency # word.text is tokenization in spacy\n",
    "    word_frequencies = {}  \n",
    "    for word in docx:\n",
    "        if word.text not in stop_words:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1\n",
    "\n",
    "\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():  \n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "    # Sentence Tokens\n",
    "    sentence_list = [ sentence for sentence in docx.sents ]\n",
    "\n",
    "    # Sentence Scores\n",
    "    sentence_scores = {}  \n",
    "    for sent in sentence_list:  \n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if len(sent.text.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "\n",
    "    summarized_sentences = nlargest(3, sentence_scores, key=sentence_scores.get)\n",
    "    final_sentences = [ w.text for w in summarized_sentences ]\n",
    "    summary = ' '.join(final_sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Partout, les eaux avaient monté, les rivières enflées, on consolidait les digues, on en bâtissait d'autres, plus hautes, mais qui se révélaient de nouveau insuffisantes. Le phénomène, inexplicable, échappait à toute logique, à toute prévision, à tout modèle, à toute saison.   Sur le port, les réfugiés se battent pour prendre place dans les derniers bateaux, pris de panique, convaincus qu'il s’agit là du dernier espoir de s'en sortir.\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summarizer('resume_pluies.txt','utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Des recettes, des anecdotes, des histoires, des cartes, des gestes, des techniques, des ustensiles, des ingrédients… En un coup d’œil, toutes les réponses aux questions que l'on se pose ! Un livre entièrement illustré pour tout savoir sur la cuisine japonaise !\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summarizer('resume_cuisine-jap.txt','utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Amour, science-fiction, polar, témoignage, aventure : tous les genres sont explorés par ces jeunes avec brio, révélant ainsi leur intérêt et leur talent pour l'écriture. Ces textes surprennent par leur fraîcheur, leur originalité, leur sincérité, et forment un kaléidoscope de l'imaginaire adolescent. Ce prix a été créé en mémoire de Clara, décédée subitement à l’âge de 13 ans des suites d’une malformation cardiaque.\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summarizer('resume_prix_clara.txt','utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=input('Entrez votre texte :')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1='Noah est beau. Cécile est joli. Les oiseaux chantent bien.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summarizer(text):\n",
    "    import re\n",
    "    from heapq import nlargest\n",
    "    docx = re.sub('\\n+',' ',text)#delete line break\n",
    "    word_frequencies = {}\n",
    "    for word in docx:\n",
    "        if word.text not in word_frequencies.keys():\n",
    "            word_frequencies[word.text] = 1\n",
    "        else:\n",
    "            word_frequencies[word.text] += 1\n",
    "\n",
    "\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "    # Sentence Tokens\n",
    "    sentence_list = [ sentence for sentence in docx.sents ]\n",
    "\n",
    "    # Sentence Scores\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if len(sent.text.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "\n",
    "    summarized_sentences = nlargest(3, sentence_scores, key=sentence_scores.get)\n",
    "    final_sentences = [ w.text for w in summarized_sentences ]\n",
    "    summary = ' '.join(final_sentences)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-279e7009c6c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_summarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-4531bf4fc7a8>\u001b[0m in \u001b[0;36mtext_summarizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mword_frequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_frequencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mword_frequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "text_summarizer(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
